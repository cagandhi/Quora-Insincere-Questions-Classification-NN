{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings.zip\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading popular: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular', quiet=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, GRU, Dense, SpatialDropout1D, Dropout, Bidirectional\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1306122 rows and 3 columns in train\n",
      "There are 375806 rows and 2 columns in test\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n",
    "test_df = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\n",
    "submission = pd.read_csv(\"../input/quora-insincere-questions-classification/sample_submission.csv\")\n",
    "\n",
    "print('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\n",
    "print('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(693466, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_df = train_df[train_df['target'] == 0]\n",
    "minority_df = train_df[train_df['target'] == 1]\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "majority_df_downsampled = resample(majority_df, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=int(0.5*majority_df.shape[0]),     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "train_df = pd.concat([majority_df_downsampled, minority_df])\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5659505497375362, 1: 4.290718970424453}\n"
     ]
    }
   ],
   "source": [
    "bincount = train_df.target.value_counts().to_numpy()\n",
    "n_samples = train_df.target.value_counts().sum()\n",
    "n_classes=2\n",
    "\n",
    "class_weights = n_samples/(n_classes*bincount)\n",
    "class_weight_dict = dict(zip(range(2), class_weights))\n",
    "\n",
    "print(class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1069272, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([train_df,test_df])\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'url',text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# def lemmatize(text):\n",
    "#     sentence_words = text.split(\" \")\n",
    "    \n",
    "#     \"\"\"\n",
    "#     l = []\n",
    "#     for word, tag in pos_tag(word_tokenize(text)):\n",
    "#         wntag = tag[0].lower()\n",
    "#         wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "\n",
    "#         if not wntag:\n",
    "#             lemma = word\n",
    "#         else:\n",
    "#             lemma = wordnet_lemmatizer.lemmatize(word, wntag)\n",
    "\n",
    "#         l.append(lemma)\n",
    "\n",
    "#     lemmatized = \" \".join(l)\n",
    "#     \"\"\"\n",
    "#     lemmatized = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in sentence_words])    \n",
    "#     return lemmatized\n",
    "\n",
    "def lower(text):\n",
    "    words = text.split(\" \")\n",
    "    lower = \" \".join([w.lower() for w in words])    \n",
    "    return lower\n",
    "\n",
    "combined_df['question_text'] = combined_df['question_text'].apply(remove_URL)\n",
    "combined_df['question_text'] = combined_df['question_text'].apply(remove_punct)\n",
    "# combined_df['question_text'] = combined_df['question_text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1069272/1069272 [04:15<00:00, 4185.14it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_corpus_new(df):\n",
    "    corpus=[]\n",
    "    for text in tqdm(df['question_text']):\n",
    "        words=[word.lower() for word in word_tokenize(text)]\n",
    "        words=' '.join(words)\n",
    "        corpus.append(words)\n",
    "    return corpus   \n",
    "\n",
    "corpus=create_corpus_new(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196016\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import zipfile\n",
    "\n",
    "dim=300\n",
    "embedding_dict={}\n",
    "\n",
    "with zipfile.ZipFile(\"../input/quora-insincere-questions-classification/embeddings.zip\") as zf:\n",
    "    with io.TextIOWrapper(zf.open(\"glove.840B.300d/glove.840B.300d.txt\"), encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values=line.split(' ') # \".split(' ')\" only for glove-840b-300d; for all other files, \".split()\" works\n",
    "            word=values[0]\n",
    "            vectors=np.asarray(values[1:],'float32')\n",
    "            embedding_dict[word]=vectors\n",
    "\n",
    "print(len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 15783/222401 [00:00<00:01, 157823.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 222401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222401/222401 [00:01<00:00, 162912.98it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 55\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(list(corpus))\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "text_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "word_index=tokenizer_obj.word_index\n",
    "\n",
    "print('Number of unique words:',len(word_index))\n",
    "num_words=len(word_index)+1\n",
    "\n",
    "embedding_matrix=np.zeros((num_words,dim))\n",
    "unknown_words = np.random.uniform(-1,1,size=dim).astype('float32')\n",
    "unknown_words = unknown_words.reshape(1,dim)\n",
    "\n",
    "for word, i in tqdm(word_index.items()):    \n",
    "    if i > num_words:\n",
    "        continue\n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec\n",
    "    else:\n",
    "        embedding_matrix[i]=unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1069272, 55)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embedding_v11.npy', embedding_matrix)\n",
    "np.save('text_pad_v11.npy', text_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (554772, 55)\n",
      "Shape of Validation  (138694, 55)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "train=text_pad[:train_df.shape[0]]\n",
    "test=text_pad[train_df.shape[0]:]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(train,train_df['target'].values,test_size=0.2,stratify=train_df['target'].values,random_state=40)\n",
    "\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam\n",
    "# model=Sequential()\n",
    "\n",
    "# embedding=Embedding(num_words,dim,embeddings_initializer=Constant(embedding_matrix),\n",
    "#                    input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "# model.add(embedding)\n",
    "# model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.15)))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',f1_m])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 55, 300)      66720600    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 55, 256)      439296      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 55, 256)      394240      bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 55, 512)      0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 67,554,649\n",
      "Trainable params: 834,049\n",
      "Non-trainable params: 66,720,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten, Input, concatenate, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "\n",
    "vec_input = Input(shape=(MAX_LEN,))\n",
    "embedding = Embedding(num_words,dim,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)(vec_input)\n",
    "bidir1 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.15, return_sequences=True))(embedding)\n",
    "bidir2 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.15, return_sequences=True))(bidir1)\n",
    "\n",
    "x = concatenate([bidir1, bidir2])\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=vec_input, outputs=output)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',f1_m])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 554772 samples, validate on 138694 samples\n",
      "Epoch 1/5\n",
      "554772/554772 [==============================] - 2921s 5ms/step - loss: 0.2753 - accuracy: 0.8838 - f1_m: 0.6407 - val_loss: 0.2630 - val_accuracy: 0.8959 - val_f1_m: 0.6657\n",
      "Epoch 2/5\n",
      "554772/554772 [==============================] - 2941s 5ms/step - loss: 0.2386 - accuracy: 0.9006 - f1_m: 0.6782 - val_loss: 0.2430 - val_accuracy: 0.9027 - val_f1_m: 0.6816\n",
      "Epoch 3/5\n",
      "554772/554772 [==============================] - 3003s 5ms/step - loss: 0.2199 - accuracy: 0.9076 - f1_m: 0.6953 - val_loss: 0.1876 - val_accuracy: 0.9261 - val_f1_m: 0.7300\n",
      "Epoch 4/5\n",
      "554772/554772 [==============================] - 2967s 5ms/step - loss: 0.2048 - accuracy: 0.9124 - f1_m: 0.7081 - val_loss: 0.2119 - val_accuracy: 0.9160 - val_f1_m: 0.7101\n",
      "Epoch 5/5\n",
      "554772/554772 [==============================] - 2943s 5ms/step - loss: 0.1917 - accuracy: 0.9170 - f1_m: 0.7205 - val_loss: 0.2192 - val_accuracy: 0.9122 - val_f1_m: 0.7028\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "batch_size=128\n",
    "history=model.fit(X_train,y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(X_test,y_test),\n",
    "                  class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('LSTM-bidirectional-128_32_32_1-bsize-128-epoch5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_GloVe = model.predict(test)\n",
    "train_pred_GloVe_int = train_pred_GloVe.round().astype('int')\n",
    "submission['prediction'] = train_pred_GloVe_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
